{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0fa450b-13b2-4c32-9dd1-ae60a44e8225",
   "metadata": {},
   "source": [
    "STAT 451: Machine Learning (Fall 2021)  \n",
    "Instructor: Sebastian Raschka (sraschka@wisc.edu)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d2da68-18e9-4c45-9158-2cdeb29a6981",
   "metadata": {
    "tags": []
   },
   "source": [
    "# L05 - Data Preprocessing and Machine Learning with Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60abc6b-9f90-42ff-9735-c17a3980114c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 5.5 Preparing Training Data & Transformer API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "202cc3ae-b00b-40db-b936-c151fdaf66ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (150, 4)\n",
      "y.shape: (150,)\n"
     ]
    }
   ],
   "source": [
    "# Code repeated from 5-2-basic-data-handling.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('data/iris.csv')\n",
    "\n",
    "d = {'Iris-setosa': 0,\n",
    "     'Iris-versicolor': 1,\n",
    "     'Iris-virginica': 2}\n",
    "df['Species'] = df['Species'].map(d)\n",
    "\n",
    "X = df.iloc[:, 1:5].values\n",
    "y = df['Species'].values\n",
    "\n",
    "\n",
    "print(f'X.shape: {X.shape}')\n",
    "print(f'y.shape: {y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12615d97-3d78-41e0-9848-abcf778cb9ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253fcbc1-f51c-4565-a8ce-fe4cf56dff3f",
   "metadata": {},
   "source": [
    "- Previously, we wrote our own code to shuffle and split a dataset into training, validation, and test subsets, which had one considerable downside.\n",
    "- If we are working with small datasets and split it randomly into subsets, it will affect the class distribution in the samples -- this is problematic since machine learning algorithms/models assume that training, validation, and test samples have been drawn from the same distributions to produce reliable models and estimates of the generalization performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe8ac80-c83b-4cae-a26a-723efd453e6a",
   "metadata": {},
   "source": [
    "<img src=\"images/iris-subsampling.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b32aa8-2f55-4905-b987-9819573ec708",
   "metadata": {},
   "source": [
    "- The method of ensuring that the class label proportions are the same in each subset after splitting, we use an approach that is usually referred to as \"stratification.\"\n",
    "- Stratification is supported in scikit-learn's `train_test_split` method if we pass the class label array to the `stratify` parameter as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9befc634-38e6-4f37-9d02-b63b3abc7fe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40, 40, 40])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_temp, X_test, y_temp, y_test = \\\n",
    "        train_test_split(X, y, test_size=0.2, \n",
    "                         shuffle=True, random_state=123, stratify=y)\n",
    "np.bincount(y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6435e3-d45e-4ad6-87e0-5fa07e561925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size (96, 4) class proportions [32 32 32]\n",
      "Valid size (24, 4) class proportions [8 8 8]\n",
      "Test size (30, 4) class proportions [10 10 10]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_valid, y_train, y_valid = \\\n",
    "        train_test_split(X_temp, y_temp, test_size=0.2,\n",
    "                         shuffle=True, random_state=123, stratify=y_temp)\n",
    "\n",
    "print('Train size', X_train.shape, 'class proportions', np.bincount(y_train))\n",
    "print('Valid size', X_valid.shape, 'class proportions', np.bincount(y_valid))\n",
    "print('Test size', X_test.shape, 'class proportions', np.bincount(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb802fc5-0987-445a-9434-01e386a9e9ef",
   "metadata": {},
   "source": [
    "## Data Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6f804-1833-4968-9404-1e9dac661deb",
   "metadata": {},
   "source": [
    "- In the case of the Iris dataset, all dimensions were measured in centimeters, hence \"scaling\" features would not be necessary in the context of *k*NN -- unless we want to weight features differently.\n",
    "- Whether or not to scale features depends on the problem at hand and requires your judgement.\n",
    "- However, there are several algorithms (especially gradient-descent, etc., which we will cover later in this course), which work much better (are more robust, numerically stable, and converge faster) if the data is centered and has a smaller range.\n",
    "- There are many different ways for scaling features; here, we only cover to of the most common \"normalization\" schemes: min-max scaling and z-score standardization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee0c04-7eb6-4df1-bb27-3cdf3d98d130",
   "metadata": {},
   "source": [
    "### Normalization -- Min-max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed1ea4c-f55f-4b60-b56c-45c7cef20c81",
   "metadata": {},
   "source": [
    "- Min-max scaling squashes the features into a [0, 1] range, which can be achieved via the following equation for a single input $i$:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34816ff-67af-448b-bfdb-3df7761aa025",
   "metadata": {},
   "source": [
    "$$x^{[i]}_{\\text{norm}} = \\frac{x^{[i]} - x_{\\text{min}} }{ x_{\\text{max}} - x_{\\text{min}} }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8dacb-91bc-41aa-91be-5c0d3bb5908b",
   "metadata": {},
   "source": [
    "- Below is an example of how we can implement and apply min-max scaling on 6 data instances given a 1D input vector (1 feature) via NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e6f35d-7ee6-46ae-a4f4-ab4f7392db8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(6).astype(float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3a4e1cd-b3c7-46b2-b986-3233abc8332e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.2, 0.4, 0.6, 0.8, 1. ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_norm = (x - x.min()) / (x.max() - x.min())\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e07d354-61b6-4e30-9047-56ad10079fe7",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e27461-ab06-47f9-ba10-1cbc955416a3",
   "metadata": {},
   "source": [
    "- Z-score standardization is a useful standardization scheme if we are working with certain optimization methods (e.g., gradient descent, later in this course). \n",
    "- After standardizing a feature, it will have the properties of a standard normal distribution, that is, unit variance and zero mean ($N(\\mu=0, \\sigma^2=1)$); however, this does not transform a feature from not following a normal distribution to a normal distributed one.\n",
    "- The formula for standardizing a feature is shown below, for a single data point $x^{[i]}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0bf6d4-5a93-472e-aab3-193624bba53e",
   "metadata": {},
   "source": [
    "$$x^{[i]}_{\\text{std}} = \\frac{x^{[i]} - \\mu_x }{ \\sigma_{x} }$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad8b1811-13e6-4598-9474-400813d167f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.arange(6).astype(float)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b1c305-d9d9-4486-bebc-f0375012e140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.46385011, -0.87831007, -0.29277002,  0.29277002,  0.87831007,\n",
       "        1.46385011])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_std = (x - x.mean()) / x.std()\n",
    "x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb952c-1207-44e0-8761-8a4ae6864e71",
   "metadata": {},
   "source": [
    "- Conveniently, NumPy and Pandas both implement a `std` method, which computes the standard devation.\n",
    "- Note the different results shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8e22e21-b019-4208-ba48-abf4d156e721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1690451944500122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([1, 2, 1, 2, 3, 4])\n",
    "df[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1978f966-425b-4f6f-b98d-29c7c50f319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0671873729054748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].values.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2deec16-b7c2-426a-b1bb-f9e86af4c387",
   "metadata": {},
   "source": [
    "- The results differ because Pandas computes the \"sample\" standard deviation ($s_x$), whereas NumPy computes the \"population\" standard deviation ($\\sigma_x$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4d20fd-1b67-41b3-8acd-c27374a08e50",
   "metadata": {},
   "source": [
    "$$s_x = \\sqrt{ \\frac{1}{n-1} \\sum^{n}_{i=1} (x^{[i]} - \\bar{x})^2 }$$\n",
    "\n",
    "$$\\sigma_x = \\sqrt{ \\frac{1}{n} \\sum^{n}_{i=1} (x^{[i]} - \\mu_x)^2 }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3de74c-df18-4af2-a95c-2e59218cf371",
   "metadata": {},
   "source": [
    "- In the context of machine learning, since we are typically working with large datasets, we typically don't care about Bessel's correction (subtracting one degree of freedom in the denominator).\n",
    "- Further, the goal here is not to model a particular distribution or estimate distribution parameters accurately; however, if you like, you can remove the extra degree of freedom via NumPy's `ddof` parameters -- it's not necessary in practice though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0573d6c-8dd5-435d-8e85-22dba6004eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1690451944500122"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0].values.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14735171-2a76-45f5-81fa-0b8752823d75",
   "metadata": {},
   "source": [
    "- A concept that is very important though is how we use the estimated normalization parameters (e.g., mean and standard deviation in z-score standardization).\n",
    "- In particular, it is important that we re-use the parameters estimated from the training set to transfrom validation and test sets -- re-estimating the parameters is a common \"beginner-mistake\" which is why we discuss it in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30f5cf68-2481-497b-878b-839287747b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu, sigma = X_train.mean(axis=0), X_train.std(axis=0)\n",
    "\n",
    "X_train_std = (X_train - mu) / sigma\n",
    "X_valid_std = (X_valid - mu) / sigma\n",
    "X_test_std = (X_test - mu) / sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b94eb5-fc4b-4fd5-9a0c-0236d5e94153",
   "metadata": {},
   "source": [
    "- Again, if we standardize the training dataset, we need to keep the parameters (mean and standard deviation for each feature). Then, we’d use these parameters to transform our test data and any future data later on\n",
    "- Let’s assume we have a simple training set consisting of 3 samples with 1 feature column (let’s call the feature column “length in cm”):\n",
    "\n",
    "- example1: 10 cm -> class 2\n",
    "- example2: 20 cm -> class 2\n",
    "- example3: 30 cm -> class 1\n",
    "\n",
    "Given the data above, we estimate the following parameters from this training set:\n",
    "\n",
    "- mean: 20\n",
    "- standard deviation: 8.2\n",
    "\n",
    "If we use these parameters to standardize the same dataset, we get the following z-score values:\n",
    "\n",
    "- example1: -1.21 -> class 2\n",
    "- example2: 0 -> class 2\n",
    "- example3: 1.21 -> class 1\n",
    "\n",
    "Now, let’s say our model has learned the following hypotheses: It classifies samples with a standardized length value < 0.6 as class 2 (and class 1 otherwise). So far so good. Now, let’s imagine we have 3 new unlabeled data points that you want to classify.\n",
    "\n",
    "- example4: 5 cm -> class ?\n",
    "- example5: 6 cm -> class ?\n",
    "- example6: 7 cm -> class ?\n",
    "\n",
    "If we look at the non-standardized \"length in cm\" values in the training datast, it is intuitive to say that all of these examples (5, 6, and 7) are likely belonging to class 2  because they are smaller than anything in the training set. However, if we standardize these by re-computing the standard deviation and and mean from the new data, we will get similar values as before (i.e., properties of a standard normal distribtion) in the training set and our classifier would (probably incorrectly) assign the “class 2” label to the samples 4 and 5.\n",
    "\n",
    "- example5: -1.21 -> class 2\n",
    "- example6: 0 -> class 2\n",
    "- example7: 1.21 -> class 1\n",
    "\n",
    "However, if we use the parameters from the \"training set standardization,\" we will get the following standardized values\n",
    "\n",
    "- example5: -18.37\n",
    "- example6: -17.15\n",
    "- example7: -15.92\n",
    "\n",
    "Note that these values are more negative than the value of example1 in the original training set, which makes much more sense now!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d43be-3fc6-4ced-b70c-3d6cf459a48a",
   "metadata": {},
   "source": [
    "### Scikit-Learn Transformer API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd00d184-e3f0-4bff-b604-c46dc48b08ca",
   "metadata": {},
   "source": [
    "- The transformer API in scikit-learn is very similar to the estimator API; the main difference is that transformers are typically \"unsupervised,\" meaning, they don't make use of class labels or target values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2f90c4-d059-4894-8553-feec2d859df4",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer-api.png\" alt=\"drawing\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3b586-d175-4855-b84d-ec2626bd907d",
   "metadata": {},
   "source": [
    "- Typical examples of transformers in scikit-learn are the `MinMaxScaler` and the `StandardScaler`, which can be used to perform min-max scaling and z-score standardization as discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c15e9356-8052-4284-be1d-f6b581274a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_valid_std = scaler.transform(X_valid)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945843f3-ff1a-4635-80ae-6f55e8dd66c2",
   "metadata": {},
   "source": [
    "## Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9bbd5-ff93-46f9-a227-39454382bae1",
   "metadata": {},
   "source": [
    "- When we preprocess a dataset as input to a machine learning algorithm, we have to be careful how we treat categorical variables.\n",
    "- There are two broad categories of categorical variables: nominal (no order implied) and ordinal (order implied)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1fa9b71-5891-494a-809f-e9fc0d9ea511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>M</td>\n",
       "      <td>10.1</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>L</td>\n",
       "      <td>13.5</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>XXL</td>\n",
       "      <td>15.3</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color size  price classlabel\n",
       "0  green    M   10.1     class1\n",
       "1    red    L   13.5     class2\n",
       "2   blue  XXL   15.3     class1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/categoricaldata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03df0f6b-5808-4f0e-97cf-7f8bd909ae12",
   "metadata": {},
   "source": [
    "- In the example above, 'size' would be an example of an ordinal variable; i.e., if the letters refer to T-shirt sizes, it would make sense to come up with an ordering like M < L < XXL.\n",
    "- Hence, we can assign increasing values to a ordinal values; however, the range and difference between categories depends on our domain knowledge and judgement.\n",
    "- To convert ordinal variables into a proper representation for numerical computations via machine learning algorithms, we can use the now familiar `map` method in Pandas, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd23b8bd-55e9-4c20-8c73-2b162b5199ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>class2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "      <td>15.3</td>\n",
       "      <td>class1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  size  price classlabel\n",
       "0  green     2   10.1     class1\n",
       "1    red     3   13.5     class2\n",
       "2   blue     5   15.3     class1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping_dict = {'M': 2,\n",
    "                'L': 3,\n",
    "                'XXL': 5}\n",
    "\n",
    "df['size'] = df['size'].map(mapping_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46552e3f-952e-4efb-b369-d773529250e9",
   "metadata": {},
   "source": [
    "- Machine learning algorithms do not assume an ordering in the case of class labels.\n",
    "- Here, we can use the `LabelEncoder` from scikit-learn to convert class labels to integers as an alternative to using the `map` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6ce438b-769e-4cbb-bd7d-1e4768c1136b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>color</th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>green</td>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>red</td>\n",
       "      <td>3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blue</td>\n",
       "      <td>5</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   color  size  price  classlabel\n",
       "0  green     2   10.1           0\n",
       "1    red     3   13.5           1\n",
       "2   blue     5   15.3           0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['classlabel'] = le.fit_transform(df['classlabel'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab67a1f8-ac3c-49db-abe6-52e68c16d011",
   "metadata": {},
   "source": [
    "- Representing nominal variables properly is a bit more tricky.\n",
    "- Since machine learning algorithms usually assume an order if a variable takes on integer values, we need to apply a \"trick\" here such that the algorithm would not make this assumption.\n",
    "- this \"trick\" is also called \"one-hot\" encoding -- we binarize a nominal variable, as shown below for the color variable (again, we do this because some ordering like orange < red < blue would not make sense in many applications)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c6d9726-ee0d-45da-96f3-b7a152696af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "      <th>color_blue</th>\n",
       "      <th>color_green</th>\n",
       "      <th>color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  price  classlabel  color_blue  color_green  color_red\n",
       "0     2   10.1           0           0            1          0\n",
       "1     3   13.5           1           0            0          1\n",
       "2     5   15.3           0           1            0          0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc4dad-2233-4abd-83df-a24ca922e1a9",
   "metadata": {},
   "source": [
    "- Note that executing the code above produced 3 new variables for \"color,\" each of which takes on binary values.\n",
    "- However, there is some redundancy now (e.g., if we know the values for `color_green` and `color_red`, we automatically know the value for `color_blue`).\n",
    "- While collinearity may cause problems (i.e., the matrix inverse doesn't exist in e.g., the context of the closed-form of linear regression), again, in machine learning we typically would not care about it too much, because most algorithms can deal with collinearity (e.g., adding constraints like regularization penalties to regression models, which we learn via gradient-based optimization).\n",
    "- However, removing collinearity if possible is never a bad idea, and we can do this conveniently by dropping e.g., one of the columns of the one-hot encoded variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d122823-9317-4e38-ab4c-ac1397755e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>size</th>\n",
       "      <th>price</th>\n",
       "      <th>classlabel</th>\n",
       "      <th>color_green</th>\n",
       "      <th>color_red</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>10.1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>13.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   size  price  classlabel  color_green  color_red\n",
       "0     2   10.1           0            1          0\n",
       "1     3   13.5           1            0          1\n",
       "2     5   15.3           0            0          0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(df, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aee4644-bef6-410f-bb19-15c63ac615b7",
   "metadata": {},
   "source": [
    "Additional categorical encoding schemes are available via the scikit-learn compatible category_encoders library: https://contrib.scikit-learn.org/category_encoders/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e04bf5-3c82-477b-828c-7c1f4f198b80",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e0eff-0303-48c1-bc75-c8bc3be0f79b",
   "metadata": {},
   "source": [
    "- There are many different ways for dealing with missing data.\n",
    "- The simplest approaches are removing entire columns or rows.\n",
    "- Another simple approach is to impute missing values via the feature means, medians, mode, etc.\n",
    "- There is no rule or best practice, and the choice of the approprite missing data imputation method depends on your judgement and domain knowledge.\n",
    "- Below are some examples for dealing with missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ae1eddb-9ac4-44d3-80d2-24d2ec8ec095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B     C    D\n",
       "0   1.0   2.0   3.0  4.0\n",
       "1   5.0   6.0   NaN  8.0\n",
       "2  10.0  11.0  12.0  NaN"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/missingdata.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1c560ae-b3ce-4583-9da3-6a3394db34c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A    0\n",
       "B    0\n",
       "C    1\n",
       "D    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values per column:\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59adf575-22fc-489a-aa62-5a37b18eb869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     A    B    C    D\n",
       "0  1.0  2.0  3.0  4.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop rows with missing values:\n",
    "\n",
    "df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd84c7c1-5118-4533-895f-547e7a054594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B\n",
       "0   1.0   2.0\n",
       "1   5.0   6.0\n",
       "2  10.0  11.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns with missing values:\n",
    "\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "742169ca-cae4-4a9b-a149-5041497e661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A     B     C    D\n",
       "0   1.0   2.0   3.0  4.0\n",
       "1   5.0   6.0   NaN  8.0\n",
       "2  10.0  11.0  12.0  NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7102ea49-919d-468c-9532-4a2825ac78fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. ,  2. ,  3. ,  4. ],\n",
       "       [ 5. ,  6. ,  7.5,  8. ],\n",
       "       [10. , 11. , 12. ,  6. ]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "X = df.values\n",
    "X = imputer.fit_transform(df.values)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c439a7b-e8c5-402c-af01-3c92c1fb0bd5",
   "metadata": {},
   "source": [
    "- Check https://scikit-learn.org/stable/modules/impute.html for additional imputation techniques, including the KNNImputer based on a k-Nearest Neighbor approach to impute missing features by nearest neighbors"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit ('base': conda)",
   "language": "python",
   "name": "python392jvsc74a57bd0249cfc85c6a0073df6bca89c83e3180d730f84f7e1f446fbe710b75104ecfa4f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
