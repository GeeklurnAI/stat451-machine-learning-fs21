{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L10: Model Evaluation 3 -- Cross-Validation and Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Last updated: 2021-11-08\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.6\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "numpy     : 1.21.2\n",
      "mlxtend   : 0.19.0\n",
      "matplotlib: 3.4.3\n",
      "sklearn   : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -a 'Sebastian Raschka' -u -d -v -p numpy,mlxtend,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "## K-fold Cross-Validation in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Simple demonstration of using a cross-validation iterator in scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([2, 3, 4, 5, 6, 7, 8, 9]), array([0, 1]))\n",
      "(array([0, 1, 4, 5, 6, 7, 8, 9]), array([2, 3]))\n",
      "(array([0, 1, 2, 3, 6, 7, 8, 9]), array([4, 5]))\n",
      "(array([0, 1, 2, 3, 4, 5, 8, 9]), array([6, 7]))\n",
      "(array([0, 1, 2, 3, 4, 5, 6, 7]), array([8, 9]))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "\n",
    "y = np.array([0, 0, 0, 0, 0, 1, 1, 1, 1, 1])\n",
    "X = rng.random_sample((y.shape[0], 4))\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5)\n",
    "\n",
    "for k in cv.split(X, y):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- In practice, we are usually interested in shuffling the dataset, because if the data records are ordered by class label, this would result in cases where the classes are not well represented in the training and test folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3, 5, 6, 7, 8, 9]), array([0, 4]))\n",
      "(array([0, 1, 2, 3, 4, 6, 8, 9]), array([5, 7]))\n",
      "(array([0, 1, 2, 4, 5, 6, 7, 9]), array([3, 8]))\n",
      "(array([0, 2, 3, 4, 5, 7, 8, 9]), array([1, 6]))\n",
      "(array([0, 1, 3, 4, 5, 6, 7, 8]), array([2, 9]))\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for k in cv.split(X, y):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Note that the `KFold` iterator only provides us with the array indices; in practice, we are actually interested in the array values (feature values and class labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels with shuffling [0 0 0 1 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 0 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n",
      "train labels with shuffling [0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "cv = KFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels with shuffling', y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- As discussed in the lecture, it's important to stratify the splits (very crucial for small datasets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n",
      "train labels [0 0 0 0 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=123, shuffle=True)\n",
    "\n",
    "for train_idx, valid_idx in cv.split(X, y):\n",
    "    print('train labels', y[train_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- After the illustrations of cross-validation above, the next cell demonstrates how we can actually use the iterators provided through scikit-learn to fit and evaluate a learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 95.26%\n",
      "Test Accuracy: 95.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mlxtend.data import iris_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = iris_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.15, \n",
    "                                                    shuffle=True, stratify=y)\n",
    "\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, random_state=123, shuffle=True)\n",
    "\n",
    "kfold_acc = 0.\n",
    "for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "    clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train[train_idx], y_train[train_idx])\n",
    "    y_pred = clf.predict(X_train[valid_idx])\n",
    "    acc = np.mean(y_pred == y_train[valid_idx])*100\n",
    "    kfold_acc += acc\n",
    "kfold_acc /= 10\n",
    "    \n",
    "clf = DecisionTreeClassifier(random_state=123, max_depth=3).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "test_acc = np.mean(y_pred == y_test)*100\n",
    "    \n",
    "print('Kfold Accuracy: %.2f%%' % kfold_acc)\n",
    "print('Test Accuracy: %.2f%%' % test_acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Usually, a more convenient way to use cross-validation through scikit-learn is to use the `cross_val_score` function (note that it performs stratifies splitting for classification by default)\n",
    "- (remember to ask students about whitespaces according to pep8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 96.09%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- `cross_val_score` has unfortunately no way to specify a random seed; this is not an issue in regular use cases, but it is not useful if you want to do \"repeated cross-validation\"\n",
    "- The next cell illustrates how we can provide our own cross-validation iterator for convenience (note that the results match or \"manual\" `StratifiedKFold` approach we performed earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kfold Accuracy: 95.26%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=StratifiedKFold(n_splits=10, random_state=123, shuffle=True),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('Kfold Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "##  Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recall Bootstrapping from 2 lectures ago? Here I is an iterator I implemented analogous to `KFold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 3 1 0 1] [2 4]\n",
      "[0 2 4 4 1] [3]\n",
      "[3 1 1 0 3] [2 4]\n",
      "[3 4 2 0 4] [1]\n",
      "[0 0 4 1 3] [2]\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import BootstrapOutOfBag\n",
    "\n",
    "oob = BootstrapOutOfBag(n_splits=5, random_seed=99)\n",
    "for train, test in oob.split(np.array([1, 2, 3, 4, 5])):\n",
    "    print(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Analagous the `KFold` iterator, we can use it in the `cross_val_score` function for convenience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Bootstrap Accuracy: 94.73%\n"
     ]
    }
   ],
   "source": [
    "cv_acc = cross_val_score(estimator=DecisionTreeClassifier(random_state=99, max_depth=3),\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=BootstrapOutOfBag(n_splits=200, random_seed=99),\n",
    "                         n_jobs=-1)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"margin-bottom:5cm;\"></p>\n",
    "\n",
    "- Analagous to the `cross_val_score` method, you can use the `bootstrap_point632_score`, which implements the .632-Bootstrap method (which is less pesimistically biased than the out-of-bag bootstrap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Bootstrap Accuracy: 95.55%\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.evaluate import bootstrap_point632_score\n",
    "\n",
    "\n",
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- By default, `bootstrap_point632_score` uses the setting `method='.632'`\n",
    "- By setting `method='.632+'`, we can also perform the .632+ bootstrap, which corrects for optimism bias, which is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Bootstrap Accuracy: 95.51%\n"
     ]
    }
   ],
   "source": [
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  method='.632+',\n",
    "                                  n_splits=200,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Finally, for your convenience, you can also set `method='oob'`, to run a regular Out-of-bag boostrap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Bootstrap Accuracy: 94.77%\n"
     ]
    }
   ],
   "source": [
    "cv_acc = bootstrap_point632_score(estimator=DecisionTreeClassifier(random_state=123, max_depth=3),\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  method='oob',\n",
    "                                  n_splits=200,\n",
    "                                  random_seed=99)\n",
    "\n",
    "print('OOB Bootstrap Accuracy: %.2f%%' % (np.mean(cv_acc)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
