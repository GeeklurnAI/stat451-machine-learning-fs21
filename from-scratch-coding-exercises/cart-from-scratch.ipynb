{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Coding Exercise \n",
    "\n",
    "## -- Implementing a \"CART\" Decision Tree From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Last updated: 2021-12-08\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.6\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "numpy     : 1.21.2\n",
      "scipy     : 1.7.0\n",
      "matplotlib: 3.4.3\n",
      "sklearn   : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Sebastian Raschka' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Implementing a \"CART\" Decision Tree from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you are going to learn how to implement the CART decision tree algorithm we discussed in class. This decision tree algorithm will construct a binary decision tree based on maximizing Information Gain using the Gini Impurity measure on continuous features.\n",
    "\n",
    "\n",
    "Implementing machine learning algorithms from scratch is a very important skill, and this homework will provide exercises that will help you to develop this skill. Even if you are interested in the more theoretical aspects of machine learning, being comfortable with implementing and trying out algorithms is vital for doing research, since even the more theoretical papers in machine learning are usually accompanied by experiments or simulations to a) verify results and b) to compare algorithms with the state-of-the art.\n",
    "\n",
    "Since many students are not expert Python programmers (yet), I will provide partial solutions to the homework tasks such that you have a framework or guide to implement the solutions. Areas that you need to fill in will be marked with comments (e.g., `# YOUR CODE`). For these partial solutions, I first implemented the functions myself, and then I deleted parts you need to fill in by these comments. However, note that you can, of course, use more or fewer lines of code than I did. In other words, all that matter is that the function you write can create the same outputs as the ones I provide. How many lines of code you need to implement that function, and how efficient it is, does not matter here. The expected outputs for the respective functions will be provided for most functions so that you can double-check your solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1) Splitting a node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we are going to implement a function that splits a dataset along a feature axis into sub-datasets. For this, we assume that the feature values are continuous (we are expecting NumPy float arrays). If the input is a NumPy integer array, we could convert it into a float array via \n",
    "\n",
    "    float_array = integer_array.astype(np.float64)\n",
    "\n",
    "To provide an intuitive example of how the splitting function should work, suppose you are given the following NumPy array with four feature values, feature values 0-3:\n",
    "\n",
    "    np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "    \n",
    "The function you are going to implement should return a dictionary, where the dictionary key stores the information about which data point goes to the left child note and which data point goes to the right child node after applying a threshold for splitting.\n",
    "\n",
    "For example, if we were to use a `split` function on the array shown above with a theshold $t=2.5$, the split function should return the following dictionary:\n",
    "\n",
    "\n",
    "    {\n",
    "     'left': array([0, 1, 3, 4, 6, 7, 8, 9]),   # smaller than or equal to threshold\n",
    "     'right': array([2, 5])                     # larger than threshold'\n",
    "     'threshold': 2.5                           # threshold for splitting, e.g., 2.5 means <= 2.5\n",
    "     }\n",
    "     \n",
    "Note that we also store a \"threshold\" key here to keep track of what value we used for the split -- we will need this later.\n",
    "\n",
    "Now it's your turn to implement the split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "\n",
    "\n",
    "def split(array, t):\n",
    "    \"\"\"\n",
    "    Function that splits a feature based on a threshold.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    array : NumPy array, type float, shape=(num_examples,)\n",
    "      A NumPy array containing feature values (float values).\n",
    "      \n",
    "    t : float\n",
    "      A threshold parameter for dividing the examples into\n",
    "      a left and a right child node.\n",
    "    \n",
    "    Returns\n",
    "    --------\n",
    "    d : dictionary of the split\n",
    "      A dictionary that has 3 keys, 'left', 'right', 'threshold'.\n",
    "      The 'threshold' simply references the threshold t we provided\n",
    "      as function argument. The 'left' child node is an integer array\n",
    "      containing the indices of the examples corresponding to feature\n",
    "      values with value <= t. The 'right' child node is an integer array\n",
    "      stores the indices of the examples for which the feature value > t.\n",
    "    \"\"\"\n",
    "    # YOUR CODE (you will likely need to write multiple lines of code)\n",
    "    d = {'left': left, 'right': right, 'threshold': t}\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'left': array([0, 1, 3, 4, 6, 7, 8, 9]), 'right': array([2, 5]), 'threshold': 2.5}\n",
      "{'left': array([0, 1, 3, 4, 6, 7, 8]), 'right': array([2, 5, 9]), 'threshold': 1.5}\n",
      "{'left': array([], dtype=int64), 'right': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), 'threshold': -0.5}\n",
      "{'left': array([0, 1, 3, 4, 6, 7, 8]), 'right': array([2, 5, 9]), 'threshold': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "print(split(ary, t=2.5))\n",
    "\n",
    "print(split(ary, t=1.5))\n",
    "\n",
    "print(split(ary, t=-0.5))\n",
    "\n",
    "print(split(ary, t=1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.2) Implement a function to compute the Gini Impurity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the splitting function, the next step is to implement a criterion function so that we can compare splits on different features. I.e., we use this criterion function to decide which feature is the best feature to split for growing the decision tree at each node. As discussed in class, our splitting criterion will be Information Gain. However, before we implement an Information Gain function, we need to implement a function that computes the Gini Impurity at each node, which we need to compute Information Gain. For your reference, we defined Gini Impurity as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$G(p) = 1 - \\sum_i (p_i)^2$$\n",
    "\n",
    "where you can think of $p_i$ as the proportion of examples with class label $i$ at a given node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "\n",
    "def gini(array):\n",
    "    \"\"\"\n",
    "    Function that computes the Gini Impurity.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    array : NumPy array, type int, shape=(num_examples,)\n",
    "      A NumPy array containing integers representing class\n",
    "      labels.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Gini impurity (float value).\n",
    "    \n",
    "    Example\n",
    "    ----------\n",
    "    >>> gini(np.array([0, 0, 1, 1]))\n",
    "    0.5\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE (you will likely need to write multiple lines of code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIP: To check your solution, try out the `gini` function on some example arrays. Note that the Gini impurity is maximum (0.5) if the classes are uniformly distributed; it is minimum if the array contains labels from only one single class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.1653\n",
      "0.0\n",
      "0.6173\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "print(round(gini(np.array([0, 1, 0, 1, 1, 0])), 4))\n",
    "print(round(gini(np.array([1, 2])), 4))\n",
    "print(round(gini(np.array([1, 1])), 4))\n",
    "print(round(gini(np.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])), 4))\n",
    "print(round(gini(np.array([0, 0, 0])), 4))\n",
    "print(round(gini(np.array([1, 1, 1, 0, 1, 4, 4, 2, 1])), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3) Implement Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a working solution for the `gini` function, the next step is to compute the Information Gain. For your reference, information gain is computed as\n",
    "\n",
    "$$GAIN(\\mathcal{D}, x_j) = H(\\mathcal{D}) - \\sum_{v \\in Values(x_j)} \\frac{|\\mathcal{D}_v|}{|\\mathcal{D}|} H(\\mathcal{D}_v).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELs\n",
    "\n",
    "\n",
    "def information_gain(x_array, y_array, split_dict):\n",
    "    \"\"\"\n",
    "    Function to compute information gain.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    \n",
    "    x_array : NumPy array, shape=(num_examples)\n",
    "      NumPy array containing the continuous feature\n",
    "      values of a given feature variable x.\n",
    "      \n",
    "    y_array : NumPy array, shape=(num_examples)\n",
    "      NumPy array containing the integer class labels\n",
    "      corresponding to the training examples.\n",
    "      \n",
    "    split_dict : dictionary\n",
    "      A dictionary created by the `split` function, which\n",
    "      contains the indices for the left and right child node.\n",
    "      \n",
    "    Returns\n",
    "    ---------\n",
    "    \n",
    "    Information gain for the given split in `split_dict`.\n",
    "    \n",
    "    \"\"\"\n",
    "    parent_gini = # YOUR CODE\n",
    "    \n",
    "    for child in ('left', 'right'):\n",
    "        \n",
    "        # TIP: freq := |D_v| / |D|\n",
    "        freq =  # YOUR CODE\n",
    "        child_gini =  # YOUR CODE\n",
    "        parent_gini -=  # YOUR CODE\n",
    "        \n",
    "    return parent_gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `information_gain` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004999999999999977\n",
      "0.003809523809523735\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x_ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "y_ary = np.array([0, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "\n",
    "split_dict_1 = split(ary, t=2.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_1))\n",
    "      \n",
    "split_dict_2 = split(ary, t=1.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_2))\n",
    "\n",
    "split_dict_3 = split(ary, t=-1.5)\n",
    "print(information_gain(x_array=x_ary, \n",
    "                       y_array=y_ary,\n",
    "                       split_dict=split_dict_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4) Creating different splitting thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should have almost all the main components that we need for implementing the CART decision tree algorithm: a `split` function, a `gini` function, and an `information_gain` function based on the `gini` function. However, since we are working with continuous feature variables, we need to find a good threshold $t$ on the number line of each feature, which we can use with our function `split`. \n",
    "\n",
    "\n",
    "For simplicity, we are going to implement a function that creates different thresholds based on the values found in a given feature variable. More precisely, we are going to implement a function `get_thresholds` that returns the lowest and highest feature value in a feature value array, plus the midpoint between each adjacent pairs of features (assuming the feature variable is sorted). \n",
    "\n",
    "\n",
    "For example, if a feature array consists of the values\n",
    "\n",
    "    [0.1, 1.2, 2.4, 2.5, 2.7, 3.3, 3.7]\n",
    "    \n",
    "the returned thresholds should be\n",
    "\n",
    "    [0.1, (0.1+1.2)/2, (1.2+2.4)/2, (2.4+2.5)/2, (2.5+2.7)/2, (2.7+3.3)/2, (3.3+3.5/2), 3.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "def get_thresholds(array):\n",
    "    \"\"\"\n",
    "    Get thresholds from a feature array.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    array : NumPy array, type float, shape=(num_examples,)\n",
    "      Array with feature values.\n",
    "      \n",
    "    Returns\n",
    "    -----------\n",
    "    NumPy float array containing thresholds.\n",
    "    \n",
    "    \"\"\"\n",
    "    # YOUR CODE (multiple lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1  0.65 1.8  2.45 2.6  3.   3.5  3.7 ]\n",
      "[0.1  0.65 1.8  2.45 2.6  3.   3.5  3.7 ]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "a = np.array([0.1, 1.2, 2.4, 2.5, 2.7, 3.3, 3.7])\n",
    "print(get_thresholds(a))\n",
    "\n",
    "b = np.array([3.7, 2.4, 1.2, 2.5, 3.3, 2.7, 0.1])\n",
    "print(get_thresholds(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5) Selecting the best splitting threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we implemented a function `get_thresholds` to create different splitting thresholds for a given feature. In this section, we are now implementing a function that selects the best threshold (the threshold that results in the largest information gain) from the array returned by `get_thresholds` by combining the \n",
    "\n",
    "- `get_thresholds`\n",
    "- `split`\n",
    "- `information_gain`\n",
    "\n",
    "functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "def get_best_threshold(x_array, y_array):\n",
    "    \"\"\"\n",
    "    Function to obtain the best threshold\n",
    "    based on maximizing information gain.\n",
    "    \n",
    "    Parameters\n",
    "    -----------\n",
    "    x_array : NumPy array, type float, shape=(num_examples,)\n",
    "      Feature array containing the feature values of a feature\n",
    "      for each training example.\n",
    "    y_array : NumPy array, type int, shape=(num_examples,)\n",
    "      NumPy array containing the class labels for each\n",
    "      training example.\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    A float representing the best threshold to split the given\n",
    "    feature variable on.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_thresholds = get_thresholds(x_array)\n",
    "    info_gains = np.zeros(all_thresholds.shape[0])\n",
    "\n",
    "    for idx, t in enumerate(all_thresholds):\n",
    "    \n",
    "        split_dict_t = # YOUR CODE\n",
    "        ig = # YOUR CODE\n",
    "\n",
    "        info_gains[idx] = ig\n",
    "        \n",
    "    best_idx = np.argmax(info_gains)\n",
    "    best_threshold = all_thresholds[best_idx]\n",
    "    \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "3.5\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "x_ary = np.array([0.0, 1.0, 4.0, 1.0, 0.0, 3.0, 1.0, 0.0, 1.0, 2.0])\n",
    "y_ary = np.array([0, 1, 1, 0, 0, 0, 1, 1, 0, 0])\n",
    "\n",
    "print(get_best_threshold(x_array=x_ary, \n",
    "                         y_array=y_ary))\n",
    "\n",
    "x_ary = np.array([0.0, 3.0, 1.0, 0.0, 1.0, 2.0, 0.0, 1.0, 4.0, 1.0,])\n",
    "y_ary = np.array([0, 0, 1, 1, 0, 0, 0, 1, 1, 0])\n",
    "\n",
    "print(get_best_threshold(x_array=x_ary, \n",
    "                         y_array=y_ary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6) Decision Tree Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to combine all the previously developed functions to recursively split a dataset on its different features to construct a decision tree that separates the examples from different classes well. We will call this function `make_tree`. \n",
    "\n",
    "For simplicity, the decision tree returned by the `make_tree` function will be represented by a Python dictionary. To illustrate this, consider the following dataset consisting of 6 training examples (class labels are 0 or 1) and 2 feature variables $X_0$ and $X_1$:\n",
    "\n",
    "```\n",
    "Inputs:\n",
    " [[0. 0.]\n",
    "  [0. 1.]\n",
    "  [1. 0.]\n",
    "  [1. 1.]\n",
    "  [2. 0.]\n",
    "  [2. 1.]]\n",
    "\n",
    "Labels:\n",
    " [0 1 0 1 1 1]\n",
    "```\n",
    " \n",
    "Based on this dataset with 6 training examples and two features, the resulting decision tree in form of the Python dictionary should look like as follows:\n",
    "\n",
    "\n",
    "\n",
    "You should return a dictionary with the following form:\n",
    "\n",
    "```\n",
    " {'X_1 <= 0.000000': {'X_0 <= 1.500000': array([0, 0]), \n",
    "                      'X_0 > 1.500000': array([1])\n",
    "                     }, \n",
    "                      \n",
    "  'X_1 > 0.000000': array([1, 1, 1])                 \n",
    " }\n",
    " ```\n",
    " \n",
    "Let me further illustrate what the different parts of the dictionary mean. Here, the `'X_1'` in `'X_1 <= 0'` refers feature 2 (the first column of the NumPy array; remember that Python starts the index at 0, in contrast to R). \n",
    "\n",
    "- 'X_1 <= 0': For training examples stored in this node where the 2nd feature is less than or equal to 0.\n",
    "- 'X_1 > 0': For training examples stored in this node where the 2nd feature is larger than 0.\n",
    "\n",
    "The \"array\" is a NumPy array that stores the class labels of the training examples at that node. In the case of `'X_1 <= 0'` we actually store actually a sub-dictionary, because this node can be split further into 2 child nodes with `'X_0 <= 1.500000'` and `'X_0 > 1.500000'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "def make_tree(X, y):\n",
    "    \"\"\"\n",
    "    A recursive function for building a binary decision tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array, type float, shape=(num_examples, num_features)\n",
    "      A design matrix representing the feature values.\n",
    "      \n",
    "    y : NumPy array, type int, shape=(num_examples,)\n",
    "      NumPy array containing the class labels corresponding to the training examples.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Dictionary representation of the decision tree.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Return array if node is empty or pure (all labels are the same or 1 example in leaf node)\n",
    "    if # YOUR CODE:\n",
    "        return y\n",
    "    \n",
    "    # Select the best threshold for each feature\n",
    "    thresholds = # YOUR CODE\n",
    "\n",
    "    # Compute information gain for each feature based on the best threshold for each feature\n",
    "    \n",
    "    gains = np.zeros(X.shape[1])\n",
    "    split_dicts = []\n",
    "\n",
    "    for idx, (feature, threshold) in enumerate(zip(X.T, thresholds)):\n",
    "        split_dict = split(feature, threshold)\n",
    "        split_dicts.append(split_dict)\n",
    "        ig = information_gain(feature, y, split_dict)\n",
    "        gains[idx] = ig\n",
    "\n",
    "    # Early stopping if there is no information gain\n",
    "    if (gains <= 1e-05).all():\n",
    "        return y\n",
    "    \n",
    "    # Else, get best feature\n",
    "    best_feature_idx = # YOUR CODE\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    subset_dict = split_dicts[best_feature_idx]\n",
    "    \n",
    "    for node in ('left', 'right'):\n",
    "        child_y_subset = y[subset_dict[node]]\n",
    "        child_X_subset = X[subset_dict[node]]\n",
    "            \n",
    "        if node == 'left':\n",
    "            results[\"X_%d <= %f\" % (best_feature_idx, subset_dict['threshold'])] = \\\n",
    "                    make_tree(child_X_subset, child_y_subset)\n",
    "            \n",
    "        else:\n",
    "            results[\"X_%d > %f\" % (best_feature_idx, subset_dict['threshold'])] = \\\n",
    "                    make_tree(child_X_subset, child_y_subset)\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " [[0. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [2. 0.]\n",
      " [2. 1.]]\n",
      "\n",
      "Labels:\n",
      " [0 1 0 1 1 1]\n",
      "\n",
      "Decision tree:\n",
      " {'X_1 <= 0.000000': {'X_0 <= 1.500000': array([0, 0]), 'X_0 > 1.500000': array([1])}, 'X_1 > 0.000000': array([1, 1, 1])}\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "x1 = np.array([0., 0., 1., 1., 2., 2.])\n",
    "x2 = np.array([0., 1., 0., 1., 0., 1.])\n",
    "X = np.array( [x1, x2]).T\n",
    "y = np.array( [0,  1,  0,  1,  1,  1])\n",
    "\n",
    "print('Inputs:\\n', X)\n",
    "print('\\nLabels:\\n', y)\n",
    "\n",
    "print('\\nDecision tree:\\n', make_tree(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7) Building a Decision Tree API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step of this part of the homework is now to write an API around our decision tree code so that we can use is for making predictions. Here, we will use the common convention, established by scikit-learn, to implement the decision tree as a Python class with \n",
    "\n",
    "- a `fit` method that learns the decision tree model from a training set via the `make_tree` function we already implemented;\n",
    "- a `predict` method to predict the class labels of training examples or any unseen data points.\n",
    "\n",
    "For making predictions, since not all leaf nodes are guaranteed to be single training examples, we will use a majority voting function to predict the class label as discussed in class. I already implemented a `_traverse` method, which will recursively traverse a decision tree dictionary that is produced by the `make_tree` function.\n",
    "\n",
    "Note that for simplicity, the `predict` method will only be able to accept one data point at a time (instead of a collection of data points). Hence `x` is a vector of size $\\mathbb{R}^m$, where $m$ is the number of features. I use capital letters `X` to denote a matrix of size $\\mathbb{R}^{n\\times m}$, where $n$ is the number of training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL\n",
    "\n",
    "\n",
    "\n",
    "class CARTDecisionTreeClassifer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.splits_ = make_tree(X, y)\n",
    "        \n",
    "    def _majority_vote(self, label_array):\n",
    "        \"\"\" Returns a class label by majority voting\n",
    "            on an array label_array\n",
    "        \"\"\"\n",
    "        return # YOUR CODE\n",
    "\n",
    "    def _traverse(self, x, d):\n",
    "        if isinstance(d, np.ndarray):\n",
    "            return d\n",
    "        for key in d:\n",
    "            \n",
    "            \n",
    "            if '<=' in key:\n",
    "                name, value = key.split(' <= ')\n",
    "                feature_idx = int(name.split('_')[-1])\n",
    "                value = float(value)\n",
    "                if x[feature_idx] <= value:\n",
    "                    return self._traverse(x, d[key])\n",
    "            else:\n",
    "                # YOUR CODE\n",
    "\n",
    "    def predict(self, x):\n",
    "        label_array = self._traverse(x, self.splits_)\n",
    "        return self._majority_vote(label_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `make_tree` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "tree = CARTDecisionTreeClassifer()\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(tree.predict(np.array([0., 0.])))\n",
    "print(tree.predict(np.array([0., 1.])))\n",
    "print(tree.predict(np.array([1., 0.])))\n",
    "print(tree.predict(np.array([1., 0.])))\n",
    "print(tree.predict(np.array([1., 1.])))\n",
    "print(tree.predict(np.array([2., 0.])))\n",
    "print(tree.predict(np.array([2., 1.])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
