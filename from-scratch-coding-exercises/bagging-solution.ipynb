{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Coding Exercise \n",
    "\n",
    "## -- Implementing a Bagging Algorithm from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Sebastian Raschka\n",
      "\n",
      "Last updated: 2021-12-17\n",
      "\n",
      "Python implementation: CPython\n",
      "Python version       : 3.9.6\n",
      "IPython version      : 7.29.0\n",
      "\n",
      "numpy     : 1.21.2\n",
      "scipy     : 1.7.0\n",
      "matplotlib: 3.4.3\n",
      "sklearn   : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark  -d -u -a 'Sebastian Raschka' -v -p numpy,scipy,matplotlib,sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this coding exercise, you will be combining multiple decision trees to a bagging classifier. This time, we will be using the decision tree algorithm implemented in scikit-learn (which is some variant of the CART algorithm for binary splits, as implemented earlier and discussed in class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Bootrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you remember, bagging relies on bootstrap sampling. So, as a first step, your task is to implement a function for generating bootstrap samples. In this exercise, for simplicity, we will perform the computations based on the Iris dataset.\n",
    "\n",
    "On an interesting side note, scikit-learn recently updated their version of the Iris dataset since it was discovered that the Iris version hosted on the UCI machine learning repository (https://archive.ics.uci.edu/ml/datasets/Iris/) has two data points that are different from R. Fisher's original paper (Fisher,R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).) and changed it in their most recent version. Since most students may not have the latest scikit-learn version installed, we will be working with the Iris dataset that is deposited on UCI, which has become quite the standard in the Python machine learning community for benchmarking algorithms. Instead of manually downloading it, we will be fetching it through the `mlxtend` (http://rasbt.github.io/mlxtend/) library that you installed in the last homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of examples: 150\n",
      "Number of features: 4\n",
      "Unique class labels: [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "from mlxtend.data import iris_data\n",
    "X, y = iris_data()\n",
    "\n",
    "print('Number of examples:', X.shape[0])\n",
    "print('Number of features:', X.shape[1])\n",
    "print('Unique class labels:', np.unique(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn's `train_test_split` function to divide the dataset into a training and a test set.\n",
    "\n",
    "- The test set should contain 45 examples, and the training set should contain 105 examples.\n",
    "- To ensure reproducible results, use `123` as a random seed.\n",
    "- Perform a stratified split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 105\n",
      "Number of test examples: 45\n"
     ]
    }
   ],
   "source": [
    "# SOLUTION\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=123,\n",
    "                                                    shuffle=True,\n",
    "                                                    stratify=y)\n",
    "\n",
    "print('Number of training examples:', X_train.shape[0])\n",
    "print('Number of test examples:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5. , 2. , 3.5, 1. ],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [4.6, 3.4, 1.4, 0.3]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we are implementing a function to generate bootstrap samples of the training set. In particular, we will perform the bootstrapping as follows:\n",
    "\n",
    "- Create an index array with values 0, ..., 104.\n",
    "- Draw a random sample (with replacement) from this index array using the `choice` method of a NumPy `RandomState` object that is passed to the function as `rng`. \n",
    "- Select training examples from the X array and labels from the y array using the new sample of indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def draw_bootstrap_sample(rng, X, y):\n",
    "    sample_indices = np.arange(X.shape[0])\n",
    "    bootstrap_indices = rng.choice(sample_indices,\n",
    "                                   size=sample_indices.shape[0],\n",
    "                                   replace=True)\n",
    "    return X[bootstrap_indices], y[bootstrap_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `draw_bootstrap_sample` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training inputs from bootstrap round: 105\n",
      "Number of training labels from bootstrap round: 105\n",
      "Labels:\n",
      " [0 0 1 0 0 1 2 0 2 1 0 0 2 1 1 1 1 2 1 1 2 0 2 1 2 1 1 1 0 1 0 0 1 2 0 0 0\n",
      " 0 2 1 1 2 1 2 1 1 2 1 2 0 1 1 2 2 1 0 1 0 2 2 0 1 0 2 0 0 0 0 1 2 0 0 1 0\n",
      " 1 1 0 1 1 2 2 0 2 0 2 0 1 1 2 2 0 2 2 2 0 1 0 1 2 2 2 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "rng = np.random.RandomState(123)\n",
    "X_boot, y_boot = draw_bootstrap_sample(rng, X_train, y_train)\n",
    "\n",
    "print('Number of training inputs from bootstrap round:', X_boot.shape[0])\n",
    "print('Number of training labels from bootstrap round:', y_boot.shape[0])\n",
    "print('Labels:\\n', y_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Baggging classifier from decision trees (4 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement a Bagging algorithm based on the `DecisionTreeClassifier`. I provided a partial solution for you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "class BaggingClassifier(object):\n",
    "    \n",
    "    def __init__(self, num_trees=10, random_state=123):\n",
    "        self.num_trees = num_trees\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.trees_ = [DecisionTreeClassifier(random_state=self.rng) for i in range(self.num_trees)]\n",
    "        for i in range(self.num_trees):\n",
    "            X_boot, y_boot = draw_bootstrap_sample(self.rng, X, y)\n",
    "            self.trees_[i].fit(X_boot, y_boot)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        ary = np.zeros((X.shape[0], len(self.trees_)), dtype=np.int64)\n",
    "        for i in range(len(self.trees_)):\n",
    "            ary[:, i] = self.trees_[i].predict(X)\n",
    "\n",
    "        maj = np.apply_along_axis(lambda x:\n",
    "                                  np.argmax(np.bincount(x)),\n",
    "                                            axis=1,\n",
    "                                            arr=ary)\n",
    "        return maj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added the following code cell for your convenience to double-check your solution. If your results don't match the results shown below, there is a bug in your implementation of the `BaggingClassifier()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual Tree Accuracies:\n",
      "88.9%\n",
      "93.3%\n",
      "97.8%\n",
      "93.3%\n",
      "93.3%\n",
      "93.3%\n",
      "91.1%\n",
      "97.8%\n",
      "97.8%\n",
      "97.8%\n",
      "\n",
      "Bagging Test Accuracy: 97.8%\n"
     ]
    }
   ],
   "source": [
    "# DO NOT EDIT OR DELETE THIS CELL\n",
    "\n",
    "model = BaggingClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "print('Individual Tree Accuracies:')\n",
    "for tree in model.trees_:\n",
    "    predictions = tree.predict(X_test) \n",
    "    print('%.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))\n",
    "\n",
    "print('\\nBagging Test Accuracy: %.1f%%' % ((predictions == y_test).sum() / X_test.shape[0] * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
